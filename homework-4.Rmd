---
title: "PSTAT231_HW04"
author: "AO XU"
date: '2022-11-01'
output: html_document
---

```{r}
library(tidyverse)
library(tidymodels)
library(corrr)
library(poissonreg)
library(ggplot2)
library(yardstick)
library(rlang)
library(corrplot)
library(discrim)
library(klaR)
library(pROC)
tidymodels_prefer()
```

```{r}
titanic = read.csv('titanic.csv')
titanic$pclass <- factor(titanic$pclass)
titanic$survived <- factor(titanic$survived, ordered=TRUE, levels=c('Yes','No'))
titanic_split <- initial_split(titanic, prop = 0.80,strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
set.seed(1588)
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + 
                           parch + fare, data = titanic_train) %>% 
  step_impute_linear(age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms= ~ starts_with("sex"):fare+
                  age:fare)
```

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r}
titanic_split <- initial_split(titanic, prop = 0.80, strata = survived)
dim(titanic_train)
dim(titanic_test)
```

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
train_folds <- vfold_cv(titanic_train, v = 10)
train_folds
```


### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

k-fold cross-validation is a re-sampling method that we split a given data set into K sections, and each section is used to test machine learning models within a limited data sample.

We should use k-fold cross-validation since it results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.

If we did use the entire training set, the re-sampling method would be LOOCV.

### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

```{r}
log_reg = logistic_reg() %>% 
        set_engine("glm") %>% 
        set_mode("classification")
log_workflow = workflow() %>% 
        add_model(log_reg) %>% 
        add_recipe(titanic_recipe)
log_fit = fit(log_workflow, titanic_train)

lda_mod = discrim_linear() %>%
        set_engine("MASS") %>%
        set_mode("classification")
lda_workflow = workflow() %>% 
        add_model(lda_mod) %>% 
        add_recipe(titanic_recipe)
lda_fit = fit(lda_workflow, titanic_train)

qda_mod = discrim_quad() %>% 
        set_mode("classification") %>% 
        set_engine("MASS")
qda_workflow = workflow() %>% 
        add_model(qda_mod) %>% 
        add_recipe(titanic_recipe)
qda_fit = fit(qda_workflow, titanic_train)
```

There are 3 models, 10 folds per model, so totally we hvae 30 folds.

### Question 5

Fit each of the models created in Question 4 to the folded data.

**IMPORTANT:** *Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of [loading and saving](https://www.r-bloggers.com/2017/04/load-save-and-rda-files/). You should still include the code to run them when you knit, but set `eval = FALSE` in the code chunks.*

```{r}
log_fit <- fit_resamples(log_workflow,train_folds)
lda_fit <- fit_resamples(lda_workflow,train_folds)
qda_fit <- fit_resamples(qda_workflow,train_folds)
```


### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

```{r}
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)
```
The logistic regression model has performed the best, since it has the highest mean accuracy - 0.8034624 than other models and has the 2nd lowest standard erro - 0.01665.

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
log_fit_entire = fit(log_workflow, titanic_train)
log_fit_entire
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

```{r}
log_pred <- predict(log_fit_entire, new_data = titanic_test, type = "class")
bind_cols(log_pred,titanic_test$survived)
train_accuracy <- augment(log_fit_entire, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
train_accuracy
test_accuracy <- augment(log_fit_entire, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
test_accuracy
```
The testing accuracy is 0.8075843 while the average accuracy is 0.8034624, so the testing accuracy is little higher than the average accuracy. 

## Required for 231 Students

Consider the following intercept-only model, with $\epsilon \sim N(0, \sigma^2)$:

$$
Y=\beta+\epsilon
$$

where $\beta$ is the parameter that we want to estimate. Suppose that we have $n$ observations of the response, i.e. $y_{1}, ..., y_{n}$, with uncorrelated errors.

### Question 9

Derive the least-squares estimate of $\beta$.

Since $Y=\beta+\epsilon$ and $\epsilon \sim N(0, \sigma^2)$, we could get that 
$RSS(\beta)= \sum_{i=1}^{N}(y_i−x_i^T\beta)^2$ = $(y−x\beta)^⊤(y−x\beta)$ 

By differentiating it with the respect to $\beta$, we could get that $x^T(y−x\beta)=0$

$X'X\beta = X'y$\

So we get that $\hat\beta = (X'X)^{-1}X'y$

### Question 10

Suppose that we perform leave-one-out cross-validation (LOOCV). Recall that, in LOOCV, we divide the data into $n$ folds. What is the covariance between $\hat{\beta}^{(1)}$, or the least-squares estimator of $\beta$ that we obtain by taking the first fold as a training set, and $\hat{\beta}^{(2)}$, the least-squares estimator of $\beta$ that we obtain by taking the second fold as a training set?

$Cov(\hat{\beta}^{(1)}, \hat{\beta_2}^{(2)}) =(X^TX)^{-1}X^T(\sigma^2I)((X^TX)^{-1}X^T)^T\\=\sigma^2(X^TX)^{-1}X^T((X^TX)^{-1}X^T)^T=\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}=\sigma^2(X^TX)^{-1}$