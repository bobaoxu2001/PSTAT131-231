---
title: "PSTAT 131 HW1"
author: "AO XU"
date: "2022-09-27"
output: html_document
---


Machine Learning Main Ideas

Problem1:
  Supervised learning is a machine learning approach that’s defined by its use of labeled datasets.
  Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled data sets.
  The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised    learning uses labeled input and output data, while an unsupervised learning algorithm does not.

Problem2:
  While regression helps predict a continuous quantity, classification predicts discrete class labels.
For Regression, Y is quantitative and numerical values; for       Classification, Y is qualitative (From lecture slides).

Problem3:
   For regression ML problems: Mean Squared Error, Mean Absolute Error
   For classification ML problem: F1 Score, AUC-ROC

Problem4:
  Descriptive models: Choose model to best visually emphasize a trend in data;
  Predictive models: Aim is to predict Y with minimum reducible error, and not focused on hypothesis tests;
  Inferential models: Aim is to test theories, (Possibly) causal claims and State relationship between outcome & predictor(s)    (From lecture slides)
  
Problem5:
  Mechanistic predictive models: uses a theory to predict what will happen in the real world.
  Empirically-driven predictive models: studies real-world events to develop a theory.
  Difference: Mechanistic predictive models assume a parametric form
for f, won't match true unknown f, and could have more flexibility by adding parameters while empirically-driven predictive models have no assumptions about f, requires a large number of observations and is much more flexible by default.
  Sameness: Both of them are overfitting.
  I think a mechanistic predictive model is easier to understand since there is a parametric form for f to model. It looks like a more direct way.
  The bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters. (From Google)
  
Problem6:
  For Question1, it's a predictive question since it is to predict Y;
  For Question2, it's an inferential question since it tests whether having the contact with candidate is important in voting choice.
  
```{r}
# Exercise1
# histogram
library(ggplot2)
data("mpg")
colors<-c("red","blue","purple","brown","pink","orange","black")
hist(mpg$hwy, col=colors, main="Highway Miles per Gallon", breaks=7, xlim = range(0:50), xlab="hwy",ylab= "numbers")
# More cars are between 15-30 highway miles per gallon, fewer cars are in 10-15 and 30-45 highway miles per gallon.
```
```{r}
# Exercise2
# scatterplot
library(ggplot2)
data("mpg")
ggplot(mpg, aes(x = hwy, y = cty)) +
  geom_point()
#I notice that as hwy increases, cty increases.
#Yes, there is a relationship between hwy and cty.
#It means that if cty if high then hwy is high and vice verca.
```
```{r}
# Exercise3
# bar plot
library(ggplot2)
data("mpg")
a <- ggplot2::mpg
a <- as.data.frame(table(a$manufacturer))
a$Var1 = as.character(a$Var1)
a
p2 <- ggplot(a,aes(x=reorder(Var1, -Freq),y=Freq))+geom_bar(stat='identity')+coord_flip()
p2
```
```{r}
# Exercise4
# box plot
library(ggplot2)
data("mpg")
boxplot(hwy ~ cyl, data = mpg,col = c("blue", "red", "yellow","green"))
```
```{r}
# Exercise5
install.packages("corrplot")
library(tidyverse)
library(corrplot)
Matrix <- ggplot2::mpg %>% select_if(is.numeric) %>% cor(.)
corrplot(Matrix, method='number',type="lower",order='alphabet',bg="grey")
# (1)displ and cyl, (2)hwy and cty, (3)year and cyl, and (4)year and displ have postive relationships.
# (1)cyl and cty, (2)displ and cty, (3)hwy and cyl, and (4)hwy and displ have negative relationships. 
# Yes, there relationship make sense to me since I think hwy and cty should be highly correlated.
# One thing surprises me is that year and hwy have no relationship.
```

```{r}
# Exercise6

```

```{r}
# Exercise7
```

```{r}
# Exercise8
```
```