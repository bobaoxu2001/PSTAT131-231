---
title: "Homework 6"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models
```{r}
rm(list = ls())
```

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

```{r}
library(tidyverse)
library(tidymodels)
library(ISLR)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(ggplot2)
library(corrr)
```

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r}
pokemon<-read.csv("/Users/xuao/Documents/2022Fall/Pstat131/PSTAT231/homework-6/data/Pokemon.csv")
pokemon<-clean_names(pokemon)

pokemon<-pokemon %>%
  filter(type_1 %in% c("Bug","Fire","Grass","Normal","Water","Psychic"))
pokemon$type_1<-factor(pokemon$type_1)
pokemon$legendary<-factor(pokemon$legendary)

pokemon_split <- initial_split(pokemon, prop = 0.80, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata=type_1)
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def,
                         data = pokemon_train) %>% 
  step_dummy(legendary,generation) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

```{r}
library(corrplot)
cor_pokemon_train <- pokemon_train %>%
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs", method = "pearson")
corrplot.mixed(cor_pokemon_train)
```
I notice that total and sp_atk have the highest-correlated relationship while defense and speed have the lowest-correlated relatinship.

Not really, I am wondering why (1) attack and total (2) sp_atk and total  are highly-correlated but attack and sp_atk are not highly-correaletd.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
tree_spec<-decision_tree() %>%
  set_engine("rpart")
class_tree_spec<-tree_spec %>%
  set_mode("classification")

tree_workflow <-workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity=tune())) %>%
  add_recipe(pokemon_recipe)

set.seed(2888)
pokemon_folds <- vfold_cv(pokemon_train)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)),levels=10)

tune_res <- tune_grid(
  tree_workflow,
  resamples=pokemon_folds,
  grid=param_grid,
  metrics=metric_set(roc_auc)
)
autoplot(tune_res)
```

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
collect_metrics(tune_res)
arrange(tune_res)
best1 <- select_best(tune_res)
best1
```
My best-performing pruned decision is model 5, the mean of roc_au is 0.6160667 which is the highest.

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
library(rpart.plot)
class_tree_final <- finalize_workflow(tree_workflow,best)
class_tree_final_fit<-fit(class_tree_final,data=pokemon_train)
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
forest <- rand_forest() %>%
  set_engine("ranger", importance="impurity") %>%
  set_mode("classification") %>% 
  set_args(mtry = tune(),trees = tune(), min_n = tune())
forest
forest_workflow <- workflow() %>%
  add_model(forest %>% set_args(mtry = tune(), trees = tune(),
                                           min_n = tune())) %>%
  add_recipe(pokemon_recipe)
#forest_workflow
param_grid2<-grid_regular(mtry(range = c(1, 8)),
                          trees(range = c(1,8)),
                          min_n(range = c(1,8)), 
                          levels = 8)
param_grid2
```

Mtry is number of predictors.

Trees is the number of trees. 

Min_n is integer for the minimum number of data points.

`mtry` should not be smaller than 1 or larger than 8 since there is at least one predictor but at most 8 predictors.

If `mtry` = 8, then it means that all predictors are used.


### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r}
#install.packages("ranger")
tune_res_forest<-tune_grid(
  forest_workflow,
  resamples = pokemon_folds,
  grid = param_grid2,
  metrics = metric_set(roc_auc)
)
autoplot(tune_res_forest)
```

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
collect_metrics(tune_res_forest) %>% 
  arrange(desc(mean))  
best2<-select_best(tune_res_forest)
best2
```
The best is Preprocessor1_Model060 and it has the highest mean 0.7155890.

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r}
class_tree_final_fit %>%
  pull_workflow_fit() %>%
  vip()
```
Sp_atk is the most useful. Generation is the least useful. Yes, it makes sense since I would care about the sp_atk the most when I play games but not care about generation too much.

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
boost_spec <- boost_tree(trees = c(10,2000), tree_depth = 4) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
param_grid_boost <- grid_regular(trees(range = c(10, 2000)),  levels = 10)

boost_workflow <- workflow() %>%
  add_model(boost_spec %>% set_args(trees = tune())) %>%
  add_recipe(pokemon_recipe)

tune_res_boost <- tune_grid(
  boost_workflow,
  resamples = pokemon_folds,
  grid = param_grid_boost,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_boost)
collect_metrics(tune_res_boost)
arrange(tune_res_boost)
best3<-select_best(tune_res_boost)
best3
```
I obeserve that there is a big increase from 0 to 250 trees for aoc_auc. But after 250 trees, the ruc_auc is around 0.715.
The best model is model 3, its roc_auc is 0.7172618.

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

```{r}
library(pROC)
# pruned tree
tree_final <- finalize_workflow(tree_workflow, best1)
tree_final_fit <- fit(class_tree_final, data = pokemon_test)

# random forest
random_forest_tree_final <- finalize_workflow(forest_workflow, best2)
rf_fit <- fit(random_forest_tree_final, data = pokemon_test)

# boosted tree
rand_tree_final <- finalize_workflow(boost_workflow, best3)
boost_fit <- fit(rand_tree_final, data = pokemon_test)

# auc and ROC curve


# roc_auc
tree_fit <- augment(tree_final_fit, 
                         new_data = pokemon_test, 
                         type = "prob") %>% 
  mutate(type_1 = as.factor(type_1)) %>% 
  roc_auc(truth = type_1, .pred_Bug:.pred_Water)

rand_tree_fit <- augment(rf_fit, 
                         new_data = pokemon_test, 
                         type = "prob") %>% 
  mutate(type_1 = as.factor(type_1)) %>% 
  roc_auc(truth = type_1, .pred_Bug:.pred_Water)

boost_model_fit <- augment(boost_fit, 
                         new_data = pokemon_test, 
                         type = "prob") %>% 
  mutate(type_1 = as.factor(type_1)) %>% 
  roc_auc(truth = type_1, .pred_Bug:.pred_Water)

matrix_rocauc <- bind_rows("ROC_AUC for the Best pruned tree model"= tree_fit,"ROC_AUC for the Best Random Forest model"=rand_tree_fit,"ROC_AUC for the Best Boosted model"=boost_model_fit,.id="model")
matrix_rocauc
```
The boosted model is the best one while the pruned tree model is the worst one.

## For 231 Students

### Exercise 11

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?
